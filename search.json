[
  {
    "objectID": "CV/index.html",
    "href": "CV/index.html",
    "title": "Curriculum vitæ",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Popov, V. & Oberauer, K. (submitted). How to build an observatory for the mind\nTrueblood, J. S., Allison, D., Field, S. M., Fishbach, A., Gaillard, S. D. M., Gigerenzer, G., … Popov, V.,… Teodorescu, A. (submitted). The Misalignment of Incentives in Academic Publishing and Implications for Journal Reform. Preprint available at OSF\nBinz, M., Alaniz, S., Roskies, A., Aczel, B., Bergstrom, C. T., Allen, C., … Popov, V. … Schulz, E. (submitted). How should the advent of large language models affect the practice of science?. Preprint available at OSF\nPopov, V. (under review). Cognitive resources can be intentionally released when processed information becomes irrelevant: Insights from the primacy effect in working memory. Preprint available at PsyArXiv\nPopov, V. (under revision). If God Handed Us the Ground-Truth Theory of Memory, How Would We Recognize It? Preprint available at OSF\nMa, S., Popov, V. , & Zhang, Q. (under revision). A Neural Index Reflecting the Amount of Cognitive Resources Available during Memory Encoding: A Model-based Approach. Preprint available at biorxiv\nFrischkorn, G.* & Popov, V.* (co-first authors) (under revision). A tutorial for estimating mix- ture models for visual working memory tasks in brms: Introducing the Bayesian Measurement Modeling (bmm) package for R. Preprint available at psyarxiv"
  },
  {
    "objectID": "publications.html#works-in-progress",
    "href": "publications.html#works-in-progress",
    "title": "Publications",
    "section": "",
    "text": "Popov, V. & Oberauer, K. (submitted). How to build an observatory for the mind\nTrueblood, J. S., Allison, D., Field, S. M., Fishbach, A., Gaillard, S. D. M., Gigerenzer, G., … Popov, V.,… Teodorescu, A. (submitted). The Misalignment of Incentives in Academic Publishing and Implications for Journal Reform. Preprint available at OSF\nBinz, M., Alaniz, S., Roskies, A., Aczel, B., Bergstrom, C. T., Allen, C., … Popov, V. … Schulz, E. (submitted). How should the advent of large language models affect the practice of science?. Preprint available at OSF\nPopov, V. (under review). Cognitive resources can be intentionally released when processed information becomes irrelevant: Insights from the primacy effect in working memory. Preprint available at PsyArXiv\nPopov, V. (under revision). If God Handed Us the Ground-Truth Theory of Memory, How Would We Recognize It? Preprint available at OSF\nMa, S., Popov, V. , & Zhang, Q. (under revision). A Neural Index Reflecting the Amount of Cognitive Resources Available during Memory Encoding: A Model-based Approach. Preprint available at biorxiv\nFrischkorn, G.* & Popov, V.* (co-first authors) (under revision). A tutorial for estimating mix- ture models for visual working memory tasks in brms: Introducing the Bayesian Measurement Modeling (bmm) package for R. Preprint available at psyarxiv"
  },
  {
    "objectID": "publications.html#journal-publications",
    "href": "publications.html#journal-publications",
    "title": "Publications",
    "section": "Journal publications",
    "text": "Journal publications\n\n\n\n\n\n\n2024\n\n\nDames, H., Musfeld, P., Popov, V., Oberauer, K., & Frischkorn, G. (in press). Responsible Research Assessment Should Prioritize Theory Development and Testing Over Ticking Open Science Boxes. Meta-psychology. Preprint available at https://psyarxiv.com/ad74m/\n\n\n\n\n2023\n\n\nDames, H. & Popov, V. (2023). When does intenet matter for memory? Bridging perspectives with Craik. Journal of Experimental Psychology: General. 152(11), 3300–3309\n\n\n\n\n\n\n\n\nPopov, V. & Dames, H. (2023). Intent Matters: Resolving the Intentional vs Incidental Learning Paradox in Episodic Long-term Memory. Journal of Experimental Psychology: General. [Preprint] [Data & Code]\n\n\n\n\n2022\n\n\nNorton, C. M., Ibinson, J. W., Pcola, S. J., Popov, V., Tremel, J. J., Reder, L. M., … & Vogt, K. M. (2022). Neutral auditory words immediately followed by painful electric shock may show reduced next-day recollection. Experimental Brain Research, 1-13.\n\n\n\n\n2021\n\n\nPopov, V., So, M. & Reder, L. (2021). Memory resources recover gradually over time: The effects of word-frequency, presentation rate and list-composition on binding errors and mnemonic precision in source memory. Journal of Experimental Psychology: Learning, Memory & Cognition. [PDF][Data & Code]\n\n\n\n\n\n\n\n\nVogt, K., Ibinson, J., Smith, C., Citro, A., Norton, C., Karim, H., Popov, V., Mahajan, A., Aizenstein, H., Reder, L. & Fiez, J. (2021). Midazolam and ketamine produce distinct neural changes in memory, pain, and fear networks during pain. Anesthesiology 135 (1), 69-82\n\n\n\n\n2020\n\n\nPopov, V. & Reder, L. (2020). Greater discrimination difficulty during perceptual learning leads to stronger and more distinct representations. Psychonomic Bulletin & Review. Advance Online Publication. [PDF] [Data & Code]\n\n\n\n\n\n\n\n\nPopov, V. & Reder, L. (2020). Frequency Effects on Memory: A Resource-Limited Theory. Psychological Review. 127(1), 1–46.  [PDF] [Preprint] [Data & Code]\n\n\n\n\n\n\n\n\nVassileva, J., Psedarska, E., Yankov, G., Bozgunov, K., Popov, V. & Vasilev, G. (2020). Validation of the Levenson Self-Report Psychopathy Scale in Bulgarian Substance Dependent Individuals. Frontiers in Psychology\n\n\n\n\n2019\n\n\nPopov, V., Marevic, I., Rummel, J. & Reder, L. (2019). Forgetting is a Feature, not a Bug: Intentionally Forgetting Some Things Helps Us Remember Others by Freeing up Working Memory Resources. Psychological Science. 30(9), 1303-1317. [PDF] [Preregistration] [[Data, Stimuli & Code]]\n\n\n\n\n\n\n\n\nPopov, V., Zhang, Q., Koch, G., Halloway, R. & Coutanche, M. (2019). Semantic knowledge influences whether novel episodic associations are represented symmetrically or asymmetrically. Memory & Cognition. Advanced Online Publication [Preregistration] [Preprint] [Stimuli, data & code]\n\n\n\n\n2018\n\n\nPopov, V.*, Ostarek, M.*, & Tenison, C. (2018). Practices and Pitfalls in Inferring Neural Representations. NeuroImage, 174, 340-351. [PDF] [Preprint] [Code]\n\n\n\n\n\n\n\n\nShen, Z.*, Popov, V.* (co-first authors), Delahay, A., & Reder, L. (2018). Item Strength Affects Working Memory Capacity. Memory & Cognition, 46(2), 204-215. [PDF]\n\n\n\n\n2017\n\n\nPopov, V., Hristova, P., & Anders, R. (2017). The Relational Luring Effect: Retrieval of relational information during associative recognition. Journal of Experimental Psychology: General, 146(5), 722-745 [PDF]\n\n\n\n\n\n\n\n\nManelis, A.*, Popov, V.* (co-first authors), Paynter, C., Walsh, M., Wheeler, M., Vogt, K., & Reder, L. (2017). Cortical Networks Involved in Memory for Temporal Order. Journal of Cognitive Neuroscience, 29(7), 1253-1266. [PDF]\n\n\n\n\n2016\n\n\nReder, L. M., Liu, X. L., Keinath, A., & Popov, V. (2016). Building knowledge requires bricks, not sand: The critical role of familiar constituents in learning. Psychonomic Bulletin & Review, 23(1), 271-277. [PDF][Data]\n\n\n\n\n2015\n\n\nPopov, V. & Hristova, P. (2015). Unintentional and efficient relational priming. Memory & Cognition, 46(6), 866-878. [PDF]"
  },
  {
    "objectID": "publications.html#refereed-full-conference-papers",
    "href": "publications.html#refereed-full-conference-papers",
    "title": "Publications",
    "section": "Refereed full conference papers",
    "text": "Refereed full conference papers\n\n\n\n\n\n\n2018\n\n\nZhang, Q.*, Popov, V.* (co-first authors), Koch, G., Halloway, R. & Coutanche, M. (2018). Fast Memory Integration Facillitated by Schema Consistency. In C. Kalish, M. Rau, J. Zhu, T. Rogers (Eds.), Proceedings of the 40th Annual Conference of the Cognitive Science Society. Austin, TX: Cognitive Science Society.\n\n\n\n\n2017\n\n\nPopov, V. , Ostarek, M., & Tenison, C. (2017). Inferential Pitfalls in Decoding Neural representations. In G. Gunzelmann, A. Howes, T. Tenbrink, & E. Davelaar (Eds.), Proceedings of the 39th Annual Conference of the Cognitive Science Society (pp. 961-966). Austin, TX: Cognitive Science Society. \n\n\n\n\n\n\n\n\nPopov, V. , & Reder, L. (2017). Target-to-distractor similarity can help visual search performance. In G. Gunzelmann, A. Howes, T. Tenbrink, & E. Davelaar (Eds.), Proceedings of the 39th Annual Conference of the Cognitive Science Society (pp. 968–973). Austin, TX: Cognitive Science Society. PDF\n\n\n\n\n\n\n\n\nPopov, V. , & Reder, L. (2017). Repetition improves memory by strengthening existing traces: Evidence from paired-associate learning under midazolam. In G. Gunzelmann, A. Howes, T. Tenbrink, & E. Davelaar (Eds.), Proceedings of the 39th Annual Conference of the Cognitive Science Society PDF (pp. 2913-2918). Austin, TX: Cognitive Science Society.\n\n\n\n\n2014\n\n\nPopov, V. & Hristova, P. (2014). Automatic analogical reasoning underlies structural priming in comprehension of ambiguous sentences. In P. Bello, M. Guarini, M. McShane, & B. Scassellati (Eds.), Proceedings of the 36th Annual Conference of the Cognitive Science Society (pp. 1192-1197). Austin, TX: Cognitive Science Society. PDF\n\n\n\n\n\n\n\n\nPopov, V. & Petkov, G. (2014). The level of processing affects the magnitude of induced retrograde amnesia. In P. Bello, M. Guarini, M. McShane, & B. Scassellati (Eds.), Proceedings of the 36th Annual Conference of the Cognitive Science Society (pp. 2787-2792). Austin, TX: Cognitive Science Society."
  },
  {
    "objectID": "publications.html#publications-in-bulgarian",
    "href": "publications.html#publications-in-bulgarian",
    "title": "Publications",
    "section": "Publications in Bulgarian",
    "text": "Publications in Bulgarian\n\n\n\n\n\n\n2016\n\n\nPopov, V., Nedelchev, D., Peneva, E., Psederska, E., Georgieva, V., Vasilev, G., Vassileva, J. (2016). Psychometric Characteristics of the Bulgarian Version of the Buss-Warren Aggression Questionnaire (BWAQ). Clinical and Consulting Psychology, 4(30), 37-53.\n\n\n\n\n\n\n\n\nPopov, V., Psederska, E., Peneva, E., Bozgunov, K., Vasilev, G., Nedelchev, D., & Vassileva, J. (2016). Psychometric Characteristics of the Bulgarian Version of the Toronto Alexithymia Scale (TAS-20). Psychological Research, 19(2), 25-42 (in Bulgarian) PDF\n\n\n\n\n\n\n\n\nNedelchev, D., Popov, V., Psedarska, E., Bozgunov, K., Vasilev, G., Peneva, E., & Vassileva, J. (2016). Psychometric Characteristics of the Bulgarian Version of the Wender Utah Rating Scale (WURS-25) for ADHD. Clinical and Consulting Psychology, 8(2), 3-17 (in Bulgarian)\n\n\n\n\n2015\n\n\nPopov, V., Bozgunov, K., Vasilev, G., & Vassileva, J. (2015). Psychometric Characteristics of the Bulgarian Version of Levenson’s Self-report Psychopathy Scale. Bulgarian Journal of Psychology, 1-4, 253-278 (in Bulgarian) PDF"
  },
  {
    "objectID": "publications.html#book-chapters",
    "href": "publications.html#book-chapters",
    "title": "Publications",
    "section": "Book chapters",
    "text": "Book chapters\nPopov, V. & Reder, M. (in press). Frequency effects in recognition and recall. To appear In M. Kahana & A. Wagner (Eds.), Handbook on Human Memory. Oxford University Press [PDF]"
  },
  {
    "objectID": "publications.html#unpublished-manuscripts",
    "href": "publications.html#unpublished-manuscripts",
    "title": "Publications",
    "section": "Unpublished manuscripts",
    "text": "Unpublished manuscripts\nPopov, V., Pavlova, M. & Hristova, P. (2020). The Internal Structure of Semantic Relations: Effects of Relational Similarity and Typicality. PsyArxiv. [Preprint]"
  },
  {
    "objectID": "posts/2024/r-multiple-package-versions/index.html",
    "href": "posts/2024/r-multiple-package-versions/index.html",
    "title": "Working with multiple versions of an R package",
    "section": "",
    "text": "Have you ever wanted to test whether your code works with multiple versions of an R package? Or compare how the behavior of certain functions has changed? There are several ways to do that, each involving a lot of setup.\n\n\nThe way to work with different package versions in base R requires you to manually specify a folder in which to install each version. Let’s assume your current version of the package stringr is 1.4.0, and you want to install separately the latest version (as of the time of this writing, 1.5.1). The following will install stringr in a folder stringr-new in your user home folder:\ndir.create('~/stringr-new')\ninstall.packages('stringr', lib = \"~/stringr-new\")\nAfterwards, you can load the default package with\nlibrary(stringr)\nand the new version with\nlibrary(stringr, lib.loc = \"~/stringr-new\")\nA few issues with this approach:\n\nYou have to manually specify the folder for each version. And remember what it was when you want to use it again.\nYou have to remember to specify the lib.loc argument every time you want to use the new version.\nYou have to remember to detach the old version before loading the new one.\nYou cannot install a specific version of a package from CRAN. You have to download the tarball from CRAN, extract it, and install it from the extracted folder.\nYou have to do this one by one for each package you want to test.\n\n\n\n\nWe can augment the base R approach with the remotes package, which provides the install_version function. This function allows you to install a specific version of a package from CRAN. The following will install stringr version 1.5.1 in a folder stringr-new in your user home folder:\ndir.create('~/stringr-new')\nremotes::install_version('stringr', version = '1.5.1', lib = \"~/stringr-new\")\nLoading the packages is the same as before. This approach solves the issue of having to download and install a specific version of a package from CRAN. However, it does not solve the other issues.\n\n\n\nThe renv package is a package manager for R. It allows you to create a project-specific library, and to specify the versions of packages you want to use in a renv.lock file. It allows for a completely reproducible environment, and is the best solution for that purpose. However, it can be an overkill if you just want to test a few versions of a package. For an introduction to renv, see this blog post."
  },
  {
    "objectID": "posts/2024/r-multiple-package-versions/index.html#the-problem",
    "href": "posts/2024/r-multiple-package-versions/index.html#the-problem",
    "title": "Working with multiple versions of an R package",
    "section": "",
    "text": "Have you ever wanted to test whether your code works with multiple versions of an R package? Or compare how the behavior of certain functions has changed? There are several ways to do that, each involving a lot of setup.\n\n\nThe way to work with different package versions in base R requires you to manually specify a folder in which to install each version. Let’s assume your current version of the package stringr is 1.4.0, and you want to install separately the latest version (as of the time of this writing, 1.5.1). The following will install stringr in a folder stringr-new in your user home folder:\ndir.create('~/stringr-new')\ninstall.packages('stringr', lib = \"~/stringr-new\")\nAfterwards, you can load the default package with\nlibrary(stringr)\nand the new version with\nlibrary(stringr, lib.loc = \"~/stringr-new\")\nA few issues with this approach:\n\nYou have to manually specify the folder for each version. And remember what it was when you want to use it again.\nYou have to remember to specify the lib.loc argument every time you want to use the new version.\nYou have to remember to detach the old version before loading the new one.\nYou cannot install a specific version of a package from CRAN. You have to download the tarball from CRAN, extract it, and install it from the extracted folder.\nYou have to do this one by one for each package you want to test.\n\n\n\n\nWe can augment the base R approach with the remotes package, which provides the install_version function. This function allows you to install a specific version of a package from CRAN. The following will install stringr version 1.5.1 in a folder stringr-new in your user home folder:\ndir.create('~/stringr-new')\nremotes::install_version('stringr', version = '1.5.1', lib = \"~/stringr-new\")\nLoading the packages is the same as before. This approach solves the issue of having to download and install a specific version of a package from CRAN. However, it does not solve the other issues.\n\n\n\nThe renv package is a package manager for R. It allows you to create a project-specific library, and to specify the versions of packages you want to use in a renv.lock file. It allows for a completely reproducible environment, and is the best solution for that purpose. However, it can be an overkill if you just want to test a few versions of a package. For an introduction to renv, see this blog post."
  },
  {
    "objectID": "posts/2024/r-multiple-package-versions/index.html#introducing-vmisc-pkg_vload",
    "href": "posts/2024/r-multiple-package-versions/index.html#introducing-vmisc-pkg_vload",
    "title": "Working with multiple versions of an R package",
    "section": "Introducing Vmisc: pkg_vload()",
    "text": "Introducing Vmisc: pkg_vload()\nI was dissatisfied with the existing solutions, so I wrote a package to do that. The Vmisc package provides the pkg_vload function, which allows you to load a specific version of a package, or to install it if it is not already installed. You can start by installing and loading the package:\ninstall.packages('Vmisc', repos = c('https://popov-lab.r-universe.dev'))\nlibrary(Vmisc)\nThe function pkg_vload combines the functionality of library(), remotes::install_version(), and dir.create(), and it also allows you to list as many packages as you want. The simplest option, for everyday use, is to specify just the package names, as you would with library():\npkg_vload(stringr, dplyr, ggplot2)\nIf you already have the package installed, it will load the default version. If you don’t, it will install the latest version from CRAN in the default library. This use case is identical to xfun::pkg_load(), but there is some added functionality for handling different versions of a package.\nTo load a specific version, you can specify the version argument:\npkg_vload(stringr('1.5.1'), dplyr, ggplot2)\nThe function expects a call to the package name, followed by the version in parentheses. It will also recognize if the version you specified is already installed. For example, if you already have stringr version 1.5.1 installed the good old way, it will load it from the default library. And you will see the following output:\n#&gt; Loading required package: stringr\n#&gt; Loading required package: dplyr\n#&gt; Attaching package: ‘dplyr’\n#&gt; Attaching package: ‘ggplot2’\nBut let’s say we want to install version 1.0.0 of stringr. We can do that with the following:\npkg_vload(stringr('1.0.0'))\nwhich results in\n#&gt; Downloading package from url: https://cran.rstudio.com//src/contrib/Archive/stringr/stringr_1.0.0.tar.gz\n#&gt; * installing *source* package 'stringr' ...\n#&gt; ** package 'stringr' successfully unpacked and MD5 sums checked\n#&gt; ** using staged installation\n#&gt; ** ## output omitted ##\n#&gt; * DONE (stringr)\n#&gt; Loading required package: stringr\npkg_vload has created a folder stringr-1.0.0 in the default library path, installed the package there, and loaded it from there. The following two folders now coexist:\ndirs = list.dirs(.libPaths(), recursive = FALSE)\ndirs[grepl('stringr', dirs)]\n#&gt; [1] \"C:/Users/vepopo/AppData/Local/R/win-library/4.3/stringr\"\n#&gt; [2] \"C:/Users/vepopo/AppData/Local/R/win-library/4.3/stringr-1.0.0\"\npkg_vload(stringr('1.0.0')) not only installed the package but also loaded it. If you restart your session, you can load either versions by simply using pkg_vload(stringr) or pkg_vload(stringr('1.0.0')).\npkg_vload(stringr('1.0.0')) # for version 1.0.0\npkg_vload(stringr) # for the default version\nBenefits of using pkg_vload:\n\nvectorized: you can load multiple packages at once\nif a package exists, it will be loaded, otherwise it will be installed and loaded\nyou can specify the version of the package you want to load/install, without having to specify the library path (although you can)\nyou can install as many versions of a package as you want, and they will coexist in the same library path\nyou can switch which version will be the default with another function from the package, pkg_switch_default"
  },
  {
    "objectID": "posts/2024/r-multiple-package-versions/index.html#switching-the-default-version-of-a-package",
    "href": "posts/2024/r-multiple-package-versions/index.html#switching-the-default-version-of-a-package",
    "title": "Working with multiple versions of an R package",
    "section": "Switching the default version of a package",
    "text": "Switching the default version of a package\nThe Vmisc package also provides a function to switch the default version of a package. For example, if you have versions 1.0.0 and 1.5.1 of stringr installed, and you want to make 1.0.0 the default, you can do that with the following:\npkg_switch_default('stringr', '1.0.0')\n#&gt; The default version of stringr has been switched to 1.0.0. The previous default version has been renamed to stringr-1.5.1\n#&gt; Please restart R to complete the process.\nWhat this will do is rename the folder stringr to stringr-1.5.1 and stringr-1.0.0 to stringr. After you restart your session, pkg_vload(stringr) or even just library(stringr) will load version 1.0.0. You can also switch back to the default version with:\npkg_switch_default('stringr', '1.5.1')\n#&gt; The default version of stringr has been switched to 1.5.1. The previous default version has been renamed to stringr-1.0.0\n#&gt; Please restart R to complete the process."
  },
  {
    "objectID": "posts/2024/r-multiple-package-versions/index.html#conclusion",
    "href": "posts/2024/r-multiple-package-versions/index.html#conclusion",
    "title": "Working with multiple versions of an R package",
    "section": "Conclusion",
    "text": "Conclusion\nAlthough not a replacement for renv for reproducible environments, Vmisc provides a simple way to work with multiple versions of an R package. It is especially useful for testing and comparing different versions of a package. The package is available on my R-universe and on GitHub. It was inspired by the xfun package, and contains other functions that I found useful in my everyday work. I hope you find it useful too."
  },
  {
    "objectID": "posts/2024/r-multiple-package-versions/index.html#bonus-finding-all-global-options-used-by-a-package",
    "href": "posts/2024/r-multiple-package-versions/index.html#bonus-finding-all-global-options-used-by-a-package",
    "title": "Working with multiple versions of an R package",
    "section": "Bonus: finding all global options used by a package",
    "text": "Bonus: finding all global options used by a package\nThe Vmisc package also provides a function to find all global options used by a package. These are options you can set via options(), but they are rarely well documented in the package documentation. The function packageOptions will list all global options used by a package, and their default values. For example, to find all global options used by the brms package, you can use the following:\npackageOptions('brms')\n\n#&gt; Package brms current options:\n#&gt; \n#&gt; brms.save_pars       :  NULL \n#&gt; mc.cores             :  1 \n#&gt; brms.threads         :  NULL \n#&gt; brms.opencl          :  NULL \n#&gt; brms.normalize       :  TRUE \n#&gt; brms.algorithm       :  \"sampling\" \n#&gt; brms.backend         :  \"rstan\" \n#&gt; future               :  FALSE \n#&gt; brms.file_refit      :  \"never\" \n#&gt; wiener_backend       :  \"Rwiener\" \n#&gt; brms.verbose         :  FALSE \n#&gt; shinystan.rstudio    :  FALSE \n#&gt; brms.plot_points     :  FALSE \n#&gt; brms.plot_rug        :  FALSE \n#&gt; brms.short_summary   :  FALSE \n#&gt; .brmsfit_version     :  NULL \nThe function is experimental - it scrapes the source code of the package to find mentions of getOption(‘something’, default = something). It also does not provide documentation. But I found it useful for reminding myself of the options I can set for a package, and their default values."
  },
  {
    "objectID": "posts/2024/taking-a-stand-on-open-peer-review/index.html",
    "href": "posts/2024/taking-a-stand-on-open-peer-review/index.html",
    "title": "Taking a stand on open peer review",
    "section": "",
    "text": "I have spent the last few months thinking about academic publishing and the peer review system. The whole thing is rotten and no sane person would design the system we have today. Now that I find myself in a professional position where I don’t have to worry about my future, I realize I can take actions to help dismantle this system. In other words, I can finally afford to act on my principles (as absurd as that sounds).\nI am not yet ready to share the full extent of what this entails, and I plan a longer post about the future of academic publishing. But last week was the first time I took action consistent with these principles, specifically my belief that all peer review should be open, published alongside the work it reviews. I received a standard review request from a respected APA journal, and this was my response:\n\nDear XXX,\nThank you for inviting me to review this submission. I would be glad to review the manuscript if my review and the subsequent author responses are published alongside the paper, should it be accepted.\nI understand this is not the current practice at XXX, but I firmly believe that open review is the future of academic publishing. It is a responsibility we owe to the public that funds our research. This is not about receiving credit for my review work but about promoting transparency and accountability in the peer review process. Many modern publishing platforms already support this practice.\nI appreciate that finding reviewers is challenging and that this request might make your job as an editor more difficult.\nBest regards,\nVen\n\nWill this have any effect? I doubt it. The system is too rigid. I expect that the editor would ask me to reconsider, and move on to other reviewers. Which is unfortunate, because I really don’t want to add more burden to a difficult job. But change has to start from somewhere.\nUpdate: As I suspected, the Editor sent me a polite email saying that the journal currently does not allow this, but that they will keep reviewing their policies as practices in the field change.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{popov2024,\n  author = {Popov, Vencislav},\n  title = {Taking a Stand on Open Peer Review},\n  date = {2024-08-04},\n  url = {https://venpopov.com/posts/2024/taking-a-stand-on-open-peer-review/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPopov, Vencislav. 2024. “Taking a Stand on Open Peer\nReview.” August 4, 2024. https://venpopov.com/posts/2024/taking-a-stand-on-open-peer-review/."
  },
  {
    "objectID": "posts/2024/reduce-friction-for-creating-quarto-blog-posts/index.html",
    "href": "posts/2024/reduce-friction-for-creating-quarto-blog-posts/index.html",
    "title": "Reduce friction for creating Quarto blog posts",
    "section": "",
    "text": "My relationship with blogging is complicated. I made my first blog about 15 years ago, fresh out of high school. WordPress was the dominant platform at the time, and my blog wasn’t anything particularly interesting - I shared some vapid attempts at poetry and short story writing, some travel logs, and a mish-mash of other topics1.\nI have never liked WordPress, for mostly the same reasons that I don’t like most software - I found every action incredibly cumbersome. Shortly thereafter I joined Facebook and Twitter, two platforms I no longer use, and most of my writing took on the “micro-blogging” nature that social media lends itself to so easily. I’ve always suspected that one of the reasons social media blew up was that it made it so easy to write, share, and follow content. At some point during my PhD (circa 2017), I created another WordPress blog, out of a desire to write longer pieces on more technical topics. That blog was again short-lived and the reason was simple - I wanted to write about programming, data analysis, and R, to share computational insights, but WordPress was a poor medium for it. At the time I was just starting to get enamored with literate programming (Jupyter notebooks were hot off the press), and the ease with which I could create computational writing contrasted starkly with the difficulty of sharing such writing on the internet.\nLast year I discovered Quarto and I immediately became excited by the possibilities. I rebuilt my personal website with it and set up this blog to go with it. I expected to post a lot more often than I have done in reality. There are many reasons for this - from starting a new tenured position, to being a typical overworked, overextended, and overcommitted academic.\nWhile I can’t do anything about that, I realized that one mental barrier arose every time I considered making a new post. Despite how easy Quarto makes it to publish technical material, there are a number of steps, at least in the way I have currently set up my system, that are just busywork, which I have to do any time I want to create a new post:\nFor some reason, the first three steps, as small as they appear, were a mental barrier that stopped me from even beginning new posts. Ideally, I want this to be as simple as possible. So I decided to write a bash script that automates it."
  },
  {
    "objectID": "posts/2024/reduce-friction-for-creating-quarto-blog-posts/index.html#a-bash-script-for-automating-post-creation",
    "href": "posts/2024/reduce-friction-for-creating-quarto-blog-posts/index.html#a-bash-script-for-automating-post-creation",
    "title": "Reduce friction for creating Quarto blog posts",
    "section": "A bash script for automating post creation",
    "text": "A bash script for automating post creation\nThe problem was that aside from some basic commands, my bash knowledge is quite limited. So I figured this was an opportunity to put Github Copilot to the test. I use Visual Studio Code as my IDE, and have setup my website repository as a workspace. I already had a markdown document that describes in plain language the steps I need to do when creating new blog posts2. So could the built-in Copilot in VSCode write me a bash script just by being told to implement the steps I’ve described in my little instruction manual?\nIt worked flawlessly, and this code was the result after a few iterations:\n#!/bin/bash\n\n# Prompt for post details\nread -p \"Enter post title: \" title\nread -p \"Enter post subtitle: \" subtitle\nread -p \"Enter post categories (comma-separated): \" categories\n\n# Convert title to a slug for the folder name\nslug=$(echo \"$title\" | tr '[:upper:]' '[:lower:]' | tr -cd '[:alnum:] ' | tr ' ' '-')\n\n# Create the new post directory\nyear=$(date +%Y)\npost_dir=\"$HOME/venpopov.com/posts/$year/$slug\"\n\n# Check if the directory already exists\nif [ -d \"$post_dir\" ]; then\n  echo \"Error: The directory $post_dir already exists.\"\n  exit 1\nfi\n\nmkdir -p \"$post_dir\"\n\n# Get today's date\ndate=$(date +%Y-%m-%d)\n\n# Append the current year to the categories list\ncategories=\"$categories, $year\"\n\n# Create the .qmd file with the provided metadata\ncat &lt;&lt;EOF &gt; \"$post_dir/index.qmd\"\n---\ntitle: \"$title\"\nsubtitle: \"$subtitle\"\ncategories: [$(echo \"$categories\" | sed 's/,/, /g')]\ndate: \"$date\"\n---\nEOF\n\necho \"New post created at $post_dir/index.qmd\"\n\n# Open the new post in the default editor\ncode \"$post_dir/index.qmd\"\nI saved this code in a file new_blog_post and Copilot instructed me that I can make it executable from the command line by first setting file permission via:\nchmod +x new_blog_post\nI saved this file in a directory on my path, so now when I want to create a new blog post, I simply open a terminal and type\nnew_blog_post\nThis prompts me to enter a title for the post, a subtitle and some tags, then creates all the necessary boilerplate and opens the file in VSCode for editing. In fact, I used it to make this very post, and for example, here is the terminal output from it\n\nNeat. Will this help me post more often? Time will tell."
  },
  {
    "objectID": "posts/2024/reduce-friction-for-creating-quarto-blog-posts/index.html#postscript-ai-and-learning-to-code",
    "href": "posts/2024/reduce-friction-for-creating-quarto-blog-posts/index.html#postscript-ai-and-learning-to-code",
    "title": "Reduce friction for creating Quarto blog posts",
    "section": "Postscript: AI and learning to code",
    "text": "Postscript: AI and learning to code\nI tell my students that they can use AI tools when working on projects, but I urge them that it is still important to learn how to code for many reasons. They need to understand what the code is doing, at the very least, and know how to fix it. Did I follow my own advice here? Ugh, maybe not, depending on the perspective. I still have no idea about the core bash syntax and how to write a script from scratch. But I do have a “lifetime” of experience with programming, and I could understand what the script was doing, and I could modify it to suit my needs. I think that’s the important part. I don’t need to know everything about bash scripting, but I need to know enough to be able to use it effectively. In any case, I have a tendency to fall into rabbit holes when learning new things, and this is the last thing I want to do right now while trying to optimize my workflow! So, I have a working script, I asked Copilot to add some reasonable checks that occurred to me, and I know what each line does, even if I don’t know how to write it from scratch. I think that’s good enough for now."
  },
  {
    "objectID": "posts/2024/reduce-friction-for-creating-quarto-blog-posts/index.html#footnotes",
    "href": "posts/2024/reduce-friction-for-creating-quarto-blog-posts/index.html#footnotes",
    "title": "Reduce friction for creating Quarto blog posts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo my huge surprise, the blog still exists! The last entry is from 2013, and I hadn’t looked at it probably since then. I only rediscovered it now while writing this post.↩︎\nI write these types of “Notes to self” instruction manuals for many things, because my memory is very poor for procedural operations↩︎"
  },
  {
    "objectID": "posts/2024/git-local-ignore/index.html",
    "href": "posts/2024/git-local-ignore/index.html",
    "title": "Locally Ignoring Git Files Without Affecting Others’ .gitignore",
    "section": "",
    "text": "I often collaborate on git projects and I find that I want to have a folder or some files stored locally in the repo but that I don’t want to be tracked by git. Obviously I could add them to .gitignore, but then I have two options:\n\ncommit the .gitignore file and push it to the repo. For public projects to which I’m contributing small changes, this is not ideal as it clutters the repo with my personal configuration (and it’s not very polite to the repo owner)\nnot commit the .gitignore file and keep it only locally. This is not ideal either, as I have to remember to not commit it every time I make a change to the repo\n\nA specific example is when I collaborate on R packages. There are several .Rprofile files which R uses to load some settings at startup. I have a bunch of convenience configurations in my user .Rprofile which helps me with my workflow. The problem is that if there is an .Rprofile file in the project root, R will use that one instead of my user .Rprofile. A workaround is to add some lines to the project .Rprofile to source my user .Rprofile, but I don’t want to commit these lines to the project .Rprofile.\nThere is an easy solution to this, but I always forget the syntax and after the 4th time I had to look it up, I decided to write it down in a blog post."
  },
  {
    "objectID": "posts/2024/git-local-ignore/index.html#the-problem",
    "href": "posts/2024/git-local-ignore/index.html#the-problem",
    "title": "Locally Ignoring Git Files Without Affecting Others’ .gitignore",
    "section": "",
    "text": "I often collaborate on git projects and I find that I want to have a folder or some files stored locally in the repo but that I don’t want to be tracked by git. Obviously I could add them to .gitignore, but then I have two options:\n\ncommit the .gitignore file and push it to the repo. For public projects to which I’m contributing small changes, this is not ideal as it clutters the repo with my personal configuration (and it’s not very polite to the repo owner)\nnot commit the .gitignore file and keep it only locally. This is not ideal either, as I have to remember to not commit it every time I make a change to the repo\n\nA specific example is when I collaborate on R packages. There are several .Rprofile files which R uses to load some settings at startup. I have a bunch of convenience configurations in my user .Rprofile which helps me with my workflow. The problem is that if there is an .Rprofile file in the project root, R will use that one instead of my user .Rprofile. A workaround is to add some lines to the project .Rprofile to source my user .Rprofile, but I don’t want to commit these lines to the project .Rprofile.\nThere is an easy solution to this, but I always forget the syntax and after the 4th time I had to look it up, I decided to write it down in a blog post."
  },
  {
    "objectID": "posts/2024/git-local-ignore/index.html#the-solution",
    "href": "posts/2024/git-local-ignore/index.html#the-solution",
    "title": "Locally Ignoring Git Files Without Affecting Others’ .gitignore",
    "section": "The Solution",
    "text": "The Solution\nThe solution depends on the state of the file.\n\nIf the file is not yet tracked by git (new file)\nIf this is a new file that is yet untracked by git, you can just add it to the local .git/info/exclude file. This file is not tracked by git and is specific to your local repo. You can add the file to this file and it will be ignored by git. This follows the same syntax as the .gitignore file. You can do this manually by opening the file and adding the file path to it, or you can do it with the following command:\necho \"&lt;file&gt;\" &gt;&gt; .git/info/exclude\nwhere &lt;file&gt; is the path to the file you want to exclude.\n\n\nIf the file is already tracked by git\nIn addition to adding the file to the local .git/info/exclude file, you also need to remove the file from the git index. This can be done with the following command:\ngit update-index --skip-worktree &lt;file&gt;\nif you change your mind and want to track this file, you can do so with the following command:\ngit update-index --no-skip-worktree &lt;file&gt;"
  },
  {
    "objectID": "posts/2024/git-local-ignore/index.html#define-an-alias-for-easy-access",
    "href": "posts/2024/git-local-ignore/index.html#define-an-alias-for-easy-access",
    "title": "Locally Ignoring Git Files Without Affecting Others’ .gitignore",
    "section": "Define an alias for easy access",
    "text": "Define an alias for easy access\nI find that I use this command often enough to warrant an alias. You can run the following commands to add an alias to your git configuration:\ngit config --global alias.ignore 'update-index --skip-worktree'\ngit config --global alias.unignore 'update-index --no-skip-worktree'\ngit config --global alias.ignored 'git ls-files -v | grep \"^S\"'\nand then you can use the following commands to ignore and unignore files:\ngit ignore &lt;file&gt;\ngit unignore &lt;file&gt;"
  },
  {
    "objectID": "posts/2024/git-local-ignore/index.html#putting-it-all-together-an-example",
    "href": "posts/2024/git-local-ignore/index.html#putting-it-all-together-an-example",
    "title": "Locally Ignoring Git Files Without Affecting Others’ .gitignore",
    "section": "Putting it all together (an example)",
    "text": "Putting it all together (an example)\nLet’s say I want to contribute code to an R package which is developed on GitHub. I can fork the repo and clone it to my local machine. The package has an .Rprofile file which overwrites my user configuration. I have a bunch of convenience configurations in my user .Rprofile which I want to use when working on this project. I can add the following lines to the project .Rprofile to source my user .Rprofile:\ntry(rprofile::load())\nI can then add the project .Rprofile to the local .git/info/exclude file:\necho \".Rprofile\" &gt;&gt; .git/info/exclude\nand finally tell git to ignore the file locally (assuming I already have the alias defined):\ngit ignore .Rprofile"
  },
  {
    "objectID": "posts/2024/introducing-bmm/index.html",
    "href": "posts/2024/introducing-bmm/index.html",
    "title": "Introducing the Bayesian Measurement Modeling R Package (bmm)",
    "section": "",
    "text": "I am excited to announce the first official release of the Bayesian Measurement Modeling R package! bmm makes Bayesian measurement modeling in psychology accessible to all. Over the past two years, Gidon Frischkorn and I worked closely together on this package and we are thrilled to finally share it with the world.\nAside from the final result, this collaboration was the most fun I’ve had doing research in a long time. I stopped counting how many days we started a random unplanned coffee chat and to suddenly realize that it’s been three hours of intense problem-solving, only because one of us was actually late for a real meeting. bmm is truly both of our baby (only slightly older that Gidon’s soon-to-be second flesh-and-blood daughter!) and we hope it will make fitting Bayesian measurement models easier, more reliable, and more efficient for everyone.\nYou can find detailed documentation, tutorials and examples on the package website. You can install bmm from CRAN as follows:"
  },
  {
    "objectID": "posts/2024/introducing-bmm/index.html#what-is-bmm-for",
    "href": "posts/2024/introducing-bmm/index.html#what-is-bmm-for",
    "title": "Introducing the Bayesian Measurement Modeling R Package (bmm)",
    "section": "What is bmm for?",
    "text": "What is bmm for?\nbmm provides a simple and intuitive interface for fitting measurement models in psychology using Bayesian methods. It is designed to be accessible to researchers with little to no experience with Bayesian statistics or cognitive modeling. If you know how to fit a mixed-effects regression model in R, you already know nearly everything you need to fit a measurement model with bmm.\n\n\n\n\n\nBut first things first: what are measurement models and why do we need them?\nIf you are a meteorologist, you can measure atmospheric temperature with a thermometer. And while thermometers are technically indirect measures, the relationship between the volume of a liquid and its temperature is so strong and so well understood that we take it for granted.\nIf you are a psychologist, things get a bit more complicated.\nBehavioral and psychological data are messy and only rough proxies for what they are supposed to measure. We rarely care about how much time it takes someone to press a button after a flashing light, how precisely they can remember a specific shade of blue, or how similar they judge two badly drawn alien animals to be. Such data are only interesting insofar as they tell us something about the underlying psychological processes that govern attention, perception, decision making, memory and categorization.\nTo make matters even more difficult, usually multiple distinct cognitive processes contribute to behavior. Combined, these issues often make it difficult to draw clear conclusions from behavioral data alone. Running t.tests on averaged raw behavioral data can only get you so far. Measurement models are an important tool to bridge the gap between latent psychological constructs and observable behavioral data.\nSuch measurement models are nowadays used in many different areas of psychology. Some of the more popular models include drift diffusion models in decision making, signal detection theory models in perception and memory, and mixture models in visual working memory. The basic idea uniting these approaches is that we can decompose one or more observed measures (e.g., reaction times, accuracy rates, angular deviation, confidence ratings) into distinct theoretically meaningful parameters that reflect latent psychological processes (e.g., decision thresholds, different memory strength signals, the quality or precision of representations). These derived parameters can then be used to test hypotheses about the underlying cognitive processes that govern behavior."
  },
  {
    "objectID": "posts/2024/introducing-bmm/index.html#the-challenges-of-measurement-modeling",
    "href": "posts/2024/introducing-bmm/index.html#the-challenges-of-measurement-modeling",
    "title": "Introducing the Bayesian Measurement Modeling R Package (bmm)",
    "section": "The challenges of measurement modeling",
    "text": "The challenges of measurement modeling\n\n\n\n\n\n\nUnfortunately, technical challenges have prevented widespread adoption of measurement models for data analysis in psychology. And even when researchers do use these models, many (*cough *) rely on ad-hoc custom implementations that are not well-documented, not well-tested, not well-understood, not easily portable to new experiments, and not using optimal inference methods. This is where bmm comes in.\n\n\n\n\n\n\nTraditionally, to fit a measurement model in psychology, you had to build it yourself. The simple reality is, however, that most researchers don’t have the time, resources, or expertise to build and fit these models from scratch. This is doubly true for Bayesian hierarchical implementations, which require a different set of tools and skills than traditional maximum likelihood estimation. And even for those who do have the skills, the process can be time-consuming and error-prone.\n\n\n\n\n\nThis is why we started working on the bmm package in the first place - we were tired of doing the same thing over and over again for every new project. What started as a personal project to make our own daily work easier has now turned into a fully-fledged R package. What we have built is a package that allows you to fit a wide range of measurement models with just a few lines of code.\nYou no longer need to:\n\ncopy-paste custom Jags or Stan code from one project to the next (or worse, try to decipher someone else’s code!) and painstakingly adjust it for your new experiment\nworry about whether you have correctly specified your priors or likelihoods\nworry about whether you have correctly implemented your model\nworry about how to adjust the script for your new experimental design and to spend hours debugging it\n\nThe bmm package takes care of all of that for you.\nWhile there exist tools for some measurement models, such as for drift diffusion models (via the wiener distribution in brms for R or via the HDDM package for Python), this is not the case for the vast majority of measurement models used in psychology. The bmm package aims to fill this gap by providing a general framework that can be extended and continuously improved by the community."
  },
  {
    "objectID": "posts/2024/introducing-bmm/index.html#example---the-interference-measurement-model",
    "href": "posts/2024/introducing-bmm/index.html#example---the-interference-measurement-model",
    "title": "Introducing the Bayesian Measurement Modeling R Package (bmm)",
    "section": "Example - the Interference Measurement Model",
    "text": "Example - the Interference Measurement Model\nTo give you a sense of how easy it is to fit a model using the bmm package, let’s walk through an example. Let’s say you have collected data from a continuous reproduction visual working memory task (see the margin for a visual representation of the task and the distribution of the data).\n\n\n\n\n\nParticipants study 1 to 8 colors in different locations. After a delay, they reproduce one of the colors as precisely as they can on a color wheel. Click image to enlarge.\n\n\n\n\n\n\n\nDensity plots of the deviation from the target color in radians, split by set size. Click image to enlarge.\n\n\n\n\nAssume you want to fit the Interference Measurement Model (IMM)(Oberauer et al. 2017) to your data. For convenience, we have included the data from this study in the bmm package, so we can use it directly and fit the model to it. The IMM model attributes errors to different sources of activation in memory: target features, non-target features, the spacial distance between them, and random noise1.\nBefore we go into the different parts, here is the entire code that you would need to fit the IMM model to the data. Yes, it’s that simple:\nlibrary(bmm)\n\n# load the data\nmy_data &lt;- oberauer_lin_2017\n\n# inform the model about the data structure\nimm_model &lt;- imm(\n  resp_error = \"dev_rad\",\n  nt_features = \"col_nt\",\n  nt_distances = \"dist_nt\",\n  set_size = \"set_size\",\n  version = \"full\",\n  regex = TRUE\n)\n\n# specify the regression formula for the model parameters\nimm_formula &lt;- bmmformula(\n  c ~ 0 + set_size + (0 + set_size | ID),\n  a ~ 0 + set_size + (0 + set_size | ID),\n  s ~ 0 + set_size + (0 + set_size | ID),\n  kappa ~ 0 + set_size + (0 + set_size | ID)\n)\n\n# fit the model via `brms` and `Stan`\nimm_fit &lt;- bmm(\n  formula = imm_formula,\n  data = my_data,\n  model = imm_model,\n  cores = 4\n)\nLet’s take a brief look at the data we are working with. In this case, the dependent variable is dev_rad - the deviation of the response from the target color (in radians). col_nt1 to col_nt7are the colors of the non-target items which were studied but are not being tested (coded relative to the target). Finally, dist_nt1 to dist_nt7 are the spatial distances of the non-target items from the target item, and set_size is the number of items in the display.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nsession\ntrial\nset_size\ndev_rad\ncol_nt1\ncol_nt2\ncol_nt3\ncol_nt4\ncol_nt5\ncol_nt6\ncol_nt7\ndist_nt1\ndist_nt2\ndist_nt3\ndist_nt4\ndist_nt5\ndist_nt6\ndist_nt7\n\n\n\n\n1\n1\n1\n7\n0.3839724\n0.8726646\n0.8552113\n2.7750735\n2.094395\n1.2566371\n0.0698132\nNA\n1.9332878\n2.416610\n1.9332878\n0.4833219\n0.9666439\n2.899932\nNA\n\n\n1\n1\n2\n3\n-0.4537856\n0.8552113\n1.9547688\nNA\nNA\nNA\nNA\nNA\n1.9332878\n2.416610\nNA\nNA\nNA\nNA\nNA\n\n\n1\n1\n3\n5\n-0.0872665\n0.7155850\n-2.6179939\n-1.0297443\n1.378810\nNA\nNA\nNA\n0.9666439\n2.899932\n2.4166097\n1.9332878\nNA\nNA\nNA\n\n\n1\n1\n4\n6\n0.3665191\n0.2617994\n2.0420352\n0.1047198\n1.099557\n-0.9250245\nNA\nNA\n0.4833219\n2.899932\n0.9666439\n0.9666439\n1.9332878\nNA\nNA\n\n\n1\n1\n5\n1\n-0.0349066\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n1\n1\n6\n1\n0.1396263\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nTo fit the IMM model to this data, we need to follow a few simple steps:\n\n1) Specify the model\nFirst, set up the bmmodel object to specify which variables in your data contain information about the identity of the target and non-target features, the distances between them, and the set size of the display. For the IMM model, this would look like this:\n\nimm_model &lt;- imm(\n  resp_error = \"dev_rad\",\n  nt_features = \"col_nt\",\n  nt_distances = \"dist_nt\",\n  set_size = \"set_size\",\n  version = \"full\",\n  regex = TRUE\n)\n\nHere we used the regex option to specify that columns which begin with col_nt and dist_nt should be treated as non-target features and distances, respectively. This is useful when you have multiple non-target features and distances in your data (instead of enumerating them all).\n\n\n2) Specify the regression formula for the model parameters\nSecond, specify how the parameters of the model should vary over different conditions. A list of all model parameters and their meaning is saved in the model object and can be accessed using the parameters element.\n\nimm_model$parameters\n#&gt; $mu1\n#&gt; Location parameter of the von Mises distribution for memory responses (in radians). Fixed internally to 0 by default.\n#&gt; \n#&gt; $kappa\n#&gt; [1] \"Concentration parameter of the von Mises distribution\"\n#&gt; \n#&gt; $a\n#&gt; [1] \"General activation of memory items\"\n#&gt; \n#&gt; $c\n#&gt; [1] \"Context activation\"\n#&gt; \n#&gt; $s\n#&gt; [1] \"Spatial similarity gradient\"\n\nUsing this information, we can set up the bmmformula for the different model parameters. Let’s say we want to first get an idea how all parameters vary across set_size:\n\nimm_formula &lt;- bmmformula(\n  c ~ 0 + set_size + (0 + set_size | ID),\n  a ~ 0 + set_size + (0 + set_size | ID),\n  s ~ 0 + set_size + (0 + set_size | ID),\n  kappa ~ 0 + set_size + (0 + set_size | ID)\n)\n\nThe bmmformula object is closely aligned with the brmsformula syntax and allows for an easy specification of grouped effects. In this case, we assume random effects for all parameters over the ID variable, thus implementing a hierarchical model estimating individual differences in all parameters across the different set sizes.\n\n\n\n\n\n\nTake a closer look at the formula object. This is where the power and flexibility of this approach comes from. You can specify any combination of fixed and random effects for any parameter in the model. This allows you to test a wide range of hypotheses about how the parameters of the model vary across different experimental conditions. In essence, fitting a hierarchical measurement model is not much different from fitting a mixed-effects regression model. You can find a more comprehensive tutorial on the bmmformula syntax and features in the online vignette\n\n\n\n\n\n3) Fit the model\nFinally, we only need to call the bmm function to estimate the model. For this, we pass the data the specified bmmodel and bmmformula to the function. In addition, we can pass additional options to the function to customize sampling (warmup, iter, chains, cores), save the fitted model object (file), or choose the backend the model should be estimated with.\nimm_fit &lt;- bmm(\n  # required inputs\n  data = my_data,\n  model = imm_model,\n  formula = imm_formula,\n  \n  # customize sampler settings\n  warmup = 1000,\n  iter = 2000,\n  chains = 4,\n  cores = 4,\n  \n  # save fitted model object\n  file = \"imm_fit\"\n)\nThe bmm package is closely integrated with brms, the leading R package for Bayesian Regression Models. This allows you to use almost any post-processing and inference method implemented for brms models with the measurement models implemented in bmm\nYou can find more detailed introductions into the different model currently implemented in bmm on the package website. And we have also written a tutorial paper that explains more details about the implementation of several measurement models, and how to specify bmmodels in different experimental settings."
  },
  {
    "objectID": "posts/2024/introducing-bmm/index.html#design-principles",
    "href": "posts/2024/introducing-bmm/index.html#design-principles",
    "title": "Introducing the Bayesian Measurement Modeling R Package (bmm)",
    "section": "Design principles",
    "text": "Design principles\nThe bmm package is built on a few key design principles, some of which were strongly inspired by the brms package for Bayesian regression modeling:\n\nSimplicity\nFitting a cognitive measurement model should be as simple as fitting a linear, logistic or a poisson regression model. You select the model you want to fit, specify a formula to predict how each parameter of the model varies across different experimental conditions, and hit go\n\n\nFlexibility\nthe package should be able to handle a wide range of measurement models and experimental designs. Any model for which you can write down a likelihood function should be able to be fit with bmm. And every parameter in the model should be able to vary across any combination of continuous and categorical predictors. This is a big departure from many existing techniques, which fit models separately to each experimental condition, forcing all parameters to vary across the same set of conditions, and preventing you from including continuous predictors. You should also be able to fix some parameters to specific values, or to impose complex constraints on them\n\n\nHierarhical estimation and one step inference\nRather than fitting each model separately to each participant, the package should allow you to estimate all parameters of the model simultaneously, while accounting for individual differences and benefit from shrinkage and sharing of information. As we detail in our tutorial paper, this has a number of advantages, including more stable estimates of population parameters, better generalization to new data, and more reliable estimates of individual differences (especially when there are few trials per participant)\n\n\nReliability and documentation\nAll models should be thoroughly tested and documented, so you can be confident that the model you are fitting is the model you think you are fitting. Each model should come with references to the relevant literature, so you can understand the theoretical background of the model and how it is implemented in bmm. The package should also provide detailed information on how to specify the model, including which parameters are available and how they can be varied across different experimental conditions\n\n\nParameter recovery\nRelated to the previous point, the package should be able to recover the parameters of the model from simulated data. These parameter recovery studies should be available as online vignettes and be fully reproducible, rather than being buried in the supplementary materials of academic papers (currently work in progress). Any trade-offs in the parameter recovery should be clearly documented, so you can understand the limitations of the model and how to interpret the results.\n\n\nEfficiency and future-proofing\nUse state-of-the-art sampling algorithms and optimization techniques to ensure that the models can be fit quickly and accurately. bmm is built on top of the brms package, which itself is an interface to the Stan probabilistic programming language. This means that the models are fit using the No-U-Turn Sampler (NUTS) algorithm, which is a state-of-the-art Hamiltonian Monte Carlo algorithm. These are packages that are actively maintained, used and supported by a large community of researchers, so that any future advances in the field of Bayesian statistics can be easily incorporated into the package as alternative backends or sampling algorithms. Furthermore, the package should be designed in a modular way, so that new models can be easily added by the community without having to change the core codebase. Finally, we have been working actively with the core developers of brms and Stan to improve sampling speed and stability of existing distributions used by our models."
  },
  {
    "objectID": "posts/2024/introducing-bmm/index.html#currently-supported-models",
    "href": "posts/2024/introducing-bmm/index.html#currently-supported-models",
    "title": "Introducing the Bayesian Measurement Modeling R Package (bmm)",
    "section": "Currently supported models",
    "text": "Currently supported models\nWe currently have the following models implemented:\nVisual working memory\n\nInterference measurement model by Oberauer and Lin (2017).\nTwo-parameter mixture model by Zhang and Luck (2008).\nThree-parameter mixture model by Bays et al (2009).\nSignal Discrimination Model (SDM) by Oberauer (2023)\n\nHowever, the setup of the bmm package provides the foundation for the implementation of a broad range of cognitive measurement models. In fact, we are already working on implementing additional models, such as:\n\nSignal-Detection Models\nEvidence Accumulation Models\nMemory Models for categorical response\n\nIf you have suggestions for models that should be added to the package, feel free to create an issue on GitHub. Ideally this should describe the model, point towards literature that gives details on the model, and if possible link to code that has already implemented the model.\nGiven the dynamic nature the bmm package is currently in, you can always view the latest list of supported models by running:\nbmm::supported_models()\nSo stay tuned for updates and new models! We hope you will find the bmm package useful and will try fitting one of the already available models to your data. We appreciate all feedback and hope that the bmm package will make the use of measurement models easier for everybody."
  },
  {
    "objectID": "posts/2024/introducing-bmm/index.html#footnotes",
    "href": "posts/2024/introducing-bmm/index.html#footnotes",
    "title": "Introducing the Bayesian Measurement Modeling R Package (bmm)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe won’t go into much detail here - you can find a detailed explanation of the IMM model and results of the model fitting in the package documentation.↩︎"
  },
  {
    "objectID": "posts/2024/reproducibility-is-hard/index.html",
    "href": "posts/2024/reproducibility-is-hard/index.html",
    "title": "Rethinking my approach to computational projects and reproducibility",
    "section": "",
    "text": "I have to finally admit it to myself - my research workflows have been breaking down and it’s time to get serious about change. I’ve been trying to incorporate better coding practices, modern reporting pipelines, and reproducible workflows into my research projects, but it’s been a struggle. The complexity of the tools, the sheer number of moving parts, and the interactions between them have been overwhelming. I’ve spent more time debugging my workflow than actually doing research. There has to be a better way."
  },
  {
    "objectID": "posts/2024/reproducibility-is-hard/index.html#the-comfort-of-old-habits",
    "href": "posts/2024/reproducibility-is-hard/index.html#the-comfort-of-old-habits",
    "title": "Rethinking my approach to computational projects and reproducibility",
    "section": "The Comfort of Old Habits",
    "text": "The Comfort of Old Habits\nFor many years I had relatively stable workflows for project organization and version control of my research projects. They were never perfect, and given the state of reproducibility in academic psychology, likely much better than the status quo. Since 2014, I’ve been using systematic project template folders, project-based organization, programmatic data wrangling, version control with Git and GitHub, and I’ve dabbled in literate programming tools such as Jupyter Notebooks and R Markdown. I’ve taugth these tools in graduate courses. I knew my way around a command line, and on the rare occasions I needed it, I could run demanding analyses on the Carnegie Mellon computing cluster. I was able to get things done efficiently while maintaining a somewhat decent level of reproducibility.\n\n\n\nI’ve been using the same project structure for 10+ years"
  },
  {
    "objectID": "posts/2024/reproducibility-is-hard/index.html#learning-to-dance",
    "href": "posts/2024/reproducibility-is-hard/index.html#learning-to-dance",
    "title": "Rethinking my approach to computational projects and reproducibility",
    "section": "Learning to Dance",
    "text": "Learning to Dance\nThis comfortable dance started changing last year, gradually at first, as I started working on my first R package. What began as a side project quickly evolved into something that preoccupied me for months. This was partly because I had a fantastic collaborator, working with whom was an exhilarating, idea-bootstrapping experience. But collaborating also meant I had to up my game — I couldn’t rely on the shortcuts and hacks I’d developed over the years. For the first time, I had to learn about proper documentation, testing, and collaborative workflows.\nWhen you’re working on a package, you’re not just writing code for yourself—you’re writing code for others to use. This means writing clear and concise documentation, creating tests to ensure your code works as expected, and developing code that is modular and easy to understand. And you have to do it in a way that makes collaboration less painful."
  },
  {
    "objectID": "posts/2024/reproducibility-is-hard/index.html#the-pitfalls-of-self-taught-coding",
    "href": "posts/2024/reproducibility-is-hard/index.html#the-pitfalls-of-self-taught-coding",
    "title": "Rethinking my approach to computational projects and reproducibility",
    "section": "The Pitfalls of Self-Taught Coding",
    "text": "The Pitfalls of Self-Taught Coding\nThe painful truth is that proper computational skills are rarely taught in academic programs (at least in psychology). Many of us are self-taught, each with our own quirky ways of doing things. Tools and processes that are standard fare in software development are often foreign to us. So, we muddle through, doing the best we can with what we have. My use of GitHub was a glorified Dropbox, and my coding practices, if you could call them that, were a mishmash of concepts I picked up from various tutorials and blog posts. It mostly worked, but even now I am barely able to reproduce my own work from a few years ago. Broken package dependencies, uncertain code order, and the utter lack of systematic documentation have made my old projects a nightmare to revisit.\n\nMaybe I’m exaggerating a bit, but the reality is that as my career has progressed, my projects have grown in number and complexity, and it’s become more and more frustrating to keep track of everything. My existing workflow sat squarely in the middle of “the reproducibility iceberg” - better than most, but I was starting to feel cold.\nThankfully, while formal education in this area still lags behind, the online landscape is now rich with resources. Over the last year, I’ve been trying to incorporate better coding practices, Quarto websites reporting for projects, and renv for package management. It’s been an uphill battle, consuming a lot of time and energy."
  },
  {
    "objectID": "posts/2024/reproducibility-is-hard/index.html#the-tension-between-efficiency-and-reproducibility",
    "href": "posts/2024/reproducibility-is-hard/index.html#the-tension-between-efficiency-and-reproducibility",
    "title": "Rethinking my approach to computational projects and reproducibility",
    "section": "The Tension Between Efficiency and Reproducibility",
    "text": "The Tension Between Efficiency and Reproducibility\nThe hardest part is not learning new tools but unlearning old habits and deconstructing my mental models of code, data, and reporting. One of the reasons I love coding in R is the incredibly quick iteration cycle and feedback loop. The ability to have an idea and, within minutes, simulate, visualize, and analyze it often feels like a superpower.\nThe problem is that this same superpower also makes it so easy to be sloppy. Who has time and patience for carefully curating a reproducible workflow when that puts a delay between your idea and its realization?\nThe thing is, I am no longer a grad student chasing down any odd idea that comes my way. As fun as the wild west of coding can be, it’s not sustainable for a long-term research program, especially when other people depend on you. And let’s be honest, I also need to be kinder to my future self."
  },
  {
    "objectID": "posts/2024/reproducibility-is-hard/index.html#what-a-mess",
    "href": "posts/2024/reproducibility-is-hard/index.html#what-a-mess",
    "title": "Rethinking my approach to computational projects and reproducibility",
    "section": "What a Mess",
    "text": "What a Mess\nCase in point: I just resumed work on a simulation project that I last touched in April. Despite all my best intentions, I was shocked to find that I couldn’t reproduce a set of figures I had sent to my collaborator at the time. Even worse, it took me a full day to even figure out what I was doing back then.\nWhy did this happen? There are many culprits, but part of it is that while I coded all the simulation scripts locally, I ran the simulations on a cluster because they were very computationally demanding. At the time, I had no established workflow for sharing intermediate data objects between my remote and local codebases. My attempts to reconstruct what had happened have driven me to the brink of madness. Trust me when I say that I’ve spent more time trying to figure out what I did than it would have taken me to redo everything from scratch.\n\n\n\nWhat a mess…\n\n\nTo avoid scenarios exactly like this, in the last six months I’ve been experimenting with the targets package. Targets is a pipeline toolkit for R that helps you manage the dependencies between your scripts and data objects. It’s a bit like make for R, but with a lot of bells and whistles. I even implemented it for a couple of other projects.\nWhen combined with renv for package management, and Quarto for reporting, it comes close to what I imagine as a nearly ideal scenario: A self-contained research website, with all the code, data, and results in one place, and a clear, reproducible workflow that can be run on any machine. A modern reporting pipeline that is both transparent and efficient."
  },
  {
    "objectID": "posts/2024/reproducibility-is-hard/index.html#the-complexity-conundrum",
    "href": "posts/2024/reproducibility-is-hard/index.html#the-complexity-conundrum",
    "title": "Rethinking my approach to computational projects and reproducibility",
    "section": "The Complexity Conundrum",
    "text": "The Complexity Conundrum\nWhen it works. Sigh.\nMaybe I’m just getting old, but putting together all these pieces has proven to be a lot more challenging than I anticipated. The learning curve is steep. These are all fantastic tools, with good documentation, and in some respects I feel lucky to be working in a time when such tools are becoming widely available, and there are so many people voluntarily developing them. The open source community is truly a marvel. But the sheer number of moving parts, and the complexity of the interactions between them, is overwhelming.\nGit + Github + renv + Quarto + targets + crew + deployment + credential management + testing + documentation + collaboration + teaching + writing + thinking + living. It’s a lot.\nOk, I got carried away with the list, but even when just considering the computational parts, each of them comes with lengthy manuals, quirks, and a dizzying amount of options and configurations, and they don’t always play nice with each other. I feel like I’m constantly fighting fires, and my self-help tutorials are getting longer and longer. These past 6 months, I’ve spent more time debugging my workflow than actually doing research.\nI’m not giving up, though. This is a path worth walking. I just can’t help but feel there must be a better way. I’ve been trying to code up a personal library of tools to automate some of that complexity, but other things keep getting in the way. I just can’t help but compare this state of affairs to the relatively much smoother package development workflows, largely thanks to devtools and usethis packages, as well as the consistent framework around it. Package development is not any less complex in scope, but the community has managed to converge on an integrated consistent workflow that just works.\nPart of this is the nature of the beast - research projects are by definition more diverse and less predictable than package development. Every project is a new adventure, with its own unique challenges and requirements, and formats vary so widely between disciplines or even subfields. Part of it is also that these tools are still relatively new."
  },
  {
    "objectID": "posts/2024/reproducibility-is-hard/index.html#the-signal-to-noise-ratio",
    "href": "posts/2024/reproducibility-is-hard/index.html#the-signal-to-noise-ratio",
    "title": "Rethinking my approach to computational projects and reproducibility",
    "section": "The Signal-to-Noise Ratio",
    "text": "The Signal-to-Noise Ratio\nBut even if all the quirks were ironed out, and the process streamlined, another issue, that I rarely see discussed, is just how much more “irrelevant” artefacts are produced in the codebase. When I look at my best attempts to implement the full workflow, I am a bit paralized by the sheer amount of files and code in my repositories that is not directly related to the substance of the research. There are layers upon layers of abstraction, scaffolding, configuration files, and helper functions that are just necessary for the workflow to function.\n\n\n\nIt’s a lot…\n\n\nTargets itself, for all its truly wonderful functionality, suffers (or at least I do!) from 1) an incredible level of syntax verbosity and 2) too many ways to do the same thing, both of which make it hard to read and understand the code. Flexibility is a double-edged sword.\nEven in an ideal world in which I finally learn all the ins and outs of these packages, I can’t help but wonder: does the improved reproducibility and transparency pay off if barely anyone else can understand what’s going on? If I share the code with expert colleagues, I doubt many of them would be able to make sense of it given the signal-to-noise ratio in the codebase. It might as well be written in a different programming language given the layers of abstraction. This perhaps is a temporary problem, as the tools mature and the community converges on best practices, but it’s a real one.\nThis is not a critique of the tools themselves, but rather a reflection on the complexity of the research process. The tools are a reflection of the complexity of the problem they are trying to solve. But there has to be a better way.\nPS: This post took an unexpected direction. I was planning to write a short introduction to my frustrations, and a detailed guide to targets as I figured out a how to apply it to a new project. I learn best by writing and teaching, and I was hoping that by writing a tutorial I would solidify my understanding of the package. As often happens in writing though, our thoughts, especially those bottled up frustrations, tend to take a life of their own. I guess I might still write the more technical post I was imagining later"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About me",
    "section": "",
    "text": "Github\n  \n  \n      Email\n  \n  \n      ORCID 0000-0002-8073-4199\n  \n  \n      Google Scholar\n  \n\n  \n  \n\n(test)\nI am a tenured Senior Scientist in Computational Modeling of Behavior at the Department of Psychology, University of Zürich. I completed my PhD in Cognitive Psychology from Carnegie Mellon University under Prof. Lynne Reder in 2020, and until 2023 I was a postdoctoral researcher with Prof. Klaus Oberauer at the University of Zürich. My research has been awarded with the 2025 Bertelson Early Career Award from the European Society for Cognitive Psychology and the 2021 Glushko Dissertation Prize from the Cognitive Science Society. I am also a consulting editor at the Memory & Cognition.\nI use computational modeling to understand the structure, organization, and function of fundamental cognitive processes, to refine the development and evaluation of psychological theories, and to improve psychological measurement.\nI publish under my full name, Vencislav Popov, but Ven is easier to say and remember, and as people have noted, “Oh, Ven, like a Venn diagram!”\n\n\nMy current research focus is on how to improve inference in psychological science to resolve the theory crisis. To that aim I develop mechanistic models that specify theoretical assumptions about how psychological processes produce behavior. So far I have done that mostly in my own field of human memory to understand how limited cognitive resources affect memory strength, but I am eager to extend such approaches to other non-cognitive phenomena. To strengthen the evaluation of theoretical models, I develop techniques for improving psychological measurement and new standards for model evaluation. To make measurement models accessible for everyone, I develop flexible and easy to use hierarchical Bayesian implementations in R. I am committed to open science, and I am strong advocate for Bayesian inference.\nSeveral goals drive my research program, including:\n\ndeveloping comprehensive computational models of human memory\nunderstanding how people control limited cognitive resources\ndeveloping and evaluating new methods for measuring cognitive processes\ndeveloping new methods and tools for computational modeling of behavior\napplying computational modeling in non-cognitive areas of psychology\nmeasuring theoretical knowledge accumulation in psychology\naddressing the theory crisis in psychology"
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "About me",
    "section": "",
    "text": "My current research focus is on how to improve inference in psychological science to resolve the theory crisis. To that aim I develop mechanistic models that specify theoretical assumptions about how psychological processes produce behavior. So far I have done that mostly in my own field of human memory to understand how limited cognitive resources affect memory strength, but I am eager to extend such approaches to other non-cognitive phenomena. To strengthen the evaluation of theoretical models, I develop techniques for improving psychological measurement and new standards for model evaluation. To make measurement models accessible for everyone, I develop flexible and easy to use hierarchical Bayesian implementations in R. I am committed to open science, and I am strong advocate for Bayesian inference.\nSeveral goals drive my research program, including:\n\ndeveloping comprehensive computational models of human memory\nunderstanding how people control limited cognitive resources\ndeveloping and evaluating new methods for measuring cognitive processes\ndeveloping new methods and tools for computational modeling of behavior\napplying computational modeling in non-cognitive areas of psychology\nmeasuring theoretical knowledge accumulation in psychology\naddressing the theory crisis in psychology"
  },
  {
    "objectID": "index.html#research-program",
    "href": "index.html#research-program",
    "title": "About me",
    "section": "Research program",
    "text": "Research program"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Computational thinking",
    "section": "",
    "text": "Reduce friction for creating Quarto blog posts\n\n\nI teach myself how to code simple shell scripts to automate some annoying tasks\n\n\n\nworkflow\n\n\nshell\n\n\n2024\n\n\n\n\n\n\n\n\n\nNov 17, 2024\n\n\n6 min\n\n\n\n\n\n\n\nRethinking my approach to computational projects and reproducibility\n\n\nThere has to be a better way\n\n\n\nWorkflow\n\n\nReproducibility\n\n\nR\n\n\ntargets\n\n\nProject management\n\n\n2024\n\n\n\n\n\n\n\n\n\nNov 17, 2024\n\n\n9 min\n\n\n\n\n\n\n\nTaking a stand on open peer review\n\n\nI will no longer review for journals that do not publish reviews alongside the work\n\n\n\nAcademic Publishing\n\n\nPeer Review\n\n\nOpen Science\n\n\n2024\n\n\n\n\n\n\n\n\n\nAug 4, 2024\n\n\n2 min\n\n\n\n\n\n\n\nIntroducing the Bayesian Measurement Modeling R Package (bmm)\n\n\nMaking Bayesian measurement modeling in psychology accessible, reliable & efficient\n\n\n\nR\n\n\nModeling\n\n\nBayesian\n\n\nR-package\n\n\n2024\n\n\n\n\n\n\n\n\n\nJun 13, 2024\n\n\n14 min\n\n\n\n\n\n\n\nLocally Ignoring Git Files Without Affecting Others’ .gitignore\n\n\nHow to exclude files from version control without affecting other developers’ .gitignore configuration\n\n\n\ngit\n\n\nGitHub\n\n\nreproducibility\n\n\nworkflow\n\n\ncollaboration\n\n\n2024\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\n4 min\n\n\n\n\n\n\n\nWorking with multiple versions of an R package\n\n\nAfter being dissatisfied with existing solutions, I wrote a package to do that\n\n\n\nR\n\n\nreproducibility\n\n\npackage management\n\n\nR-package\n\n\n2024\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  }
]